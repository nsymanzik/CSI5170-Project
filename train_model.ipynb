{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from skopt import BayesSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs shape: (2655, 514)\n",
      "Outputs shape: (2655, 3)\n"
     ]
    }
   ],
   "source": [
    "# Load JSON files\n",
    "with open(os.path.abspath('./hdr_data/labels.json'), 'r') as f:\n",
    "    scenes = json.load(f)\n",
    "\n",
    "with open(os.path.abspath('./hdr_data/histograms.json'), 'r') as f:\n",
    "    histograms = json.load(f)\n",
    "\n",
    "# Create a dictionary to match scenes to histograms\n",
    "scene_histograms = {}\n",
    "for item in histograms:\n",
    "    scene_name = item['scene']\n",
    "    if scene_name not in scene_histograms:\n",
    "        scene_histograms[scene_name] = []\n",
    "    scene_histograms[scene_name].append({'image': item['image'], 'histogram': item['histogram']})\n",
    "\n",
    "data = []\n",
    "exposure_threshold = 100000  # 100 ms in microseconds\n",
    "\n",
    "for scene in scenes:\n",
    "    scene_name = scene['scene']\n",
    "    best_exposures = scene['best_exposure_times']\n",
    "\n",
    "    if scene_name in scene_histograms:\n",
    "        hist_data = scene_histograms[scene_name]\n",
    "        available_images = []\n",
    "\n",
    "        # Filter images based on exposure time\n",
    "        for image_entry in hist_data:\n",
    "            image_name = image_entry['image']\n",
    "            exposure_time = float(image_name.split('.')[0])  # Extract exposure time\n",
    "            histogram = image_entry['histogram']\n",
    "\n",
    "            if exposure_time < exposure_threshold:\n",
    "                available_images.append((np.array(histogram).flatten(), exposure_time))\n",
    "\n",
    "        # Create combinations of two images for input\n",
    "        for hist_1, hist_2 in combinations(available_images, 2):\n",
    "            histogram_1, exposure_1 = hist_1\n",
    "            histogram_2, exposure_2 = hist_2\n",
    "\n",
    "            # Combine inputs\n",
    "            input_data = np.concatenate([histogram_1, histogram_2, [exposure_1], [exposure_2]])\n",
    "            output_data = np.array(best_exposures)\n",
    "\n",
    "            data.append((input_data, output_data))\n",
    "\n",
    "# Convert to NumPy arrays for training\n",
    "inputs = np.array([item[0] for item in data])\n",
    "outputs = np.array([item[1] for item in data])\n",
    "\n",
    "# Print the shapes of the inputs and outputs to verify\n",
    "print(f'Inputs shape: {inputs.shape}')\n",
    "print(f'Outputs shape: {outputs.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 10 candidates, totalling 20 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Validation Mean Absolute Error: 19441.971843787625\n",
      "Validation R^2 Score: 0.9309927565833758\n",
      "Best Hyperparameters: {'n_estimators': 300, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': 30}\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(inputs, outputs, test_size=0.2, random_state=1)\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['log2', 'sqrt']\n",
    "}\n",
    "\n",
    "# Create a RandomForestRegressor instance\n",
    "rf = RandomForestRegressor(random_state=1)\n",
    "\n",
    "# Perform Randomized Search to find the best hyperparameters\n",
    "random_search = RandomizedSearchCV(estimator=rf, param_distributions=param_grid, n_iter=10, cv=2, n_jobs=-1, verbose=3, scoring='neg_mean_absolute_error', random_state=1)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best estimator from random search\n",
    "best_rf = random_search.best_estimator_\n",
    "\n",
    "# Train the best Random Forest model\n",
    "best_rf.fit(X_train, y_train)\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': list(range(10, 300, 10)),\n",
    "    'max_depth': list(range(5, 50, 5)),\n",
    "    'min_samples_split': list(range(2, 10)),\n",
    "    'min_samples_leaf': list(range(1, 5)),\n",
    "    'max_features': ['log2', 'sqrt']\n",
    "}\n",
    "\n",
    "# Perform Bayesian Optimization to further improve hyperparameters\n",
    "bayes_search = BayesSearchCV(estimator=best_rf, search_spaces=param_grid, n_iter=30, cv=3, n_jobs=-1, scoring='neg_mean_absolute_error', random_state=1)\n",
    "bayes_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best estimator from Bayesian search\n",
    "best_rf_bayes = bayes_search.best_estimator_\n",
    "\n",
    "# Train the best Random Forest model from Bayesian Optimization\n",
    "best_rf_bayes.fit(X_train, y_train)\n",
    "\n",
    "# Validate the model\n",
    "y_pred = best_rf_bayes.predict(X_val)\n",
    "mae = mean_absolute_error(y_val, y_pred)\n",
    "r2 = r2_score(y_val, y_pred)\n",
    "print(f\"Validation Mean Absolute Error: {mae}\")\n",
    "print(f\"Validation R^2 Score: {r2}\")\n",
    "\n",
    "# Print the best parameters found by Random Search\n",
    "print(\"Best Hyperparameters:\", random_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted exposure times for scene scene_0011: [[ 1177.29204545  3492.04545455 50936.36363636]]\n",
      "Ground-truth exposure times for scene scene_0011: [187.0, 1500.0, 6000.0]\n",
      "Percentage errors for each exposure time: [529.5679387457461, 132.80303030303028, 748.939393939394]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Select a specific scene and get available inputs for that scene\n",
    "scene_name = 'scene_0011'  # Replace with the specific scene you want to test\n",
    "\n",
    "# Find the data points that belong to the specific scene\n",
    "scene_inputs = [input_data for (input_data, output_data), scene in zip(data, scenes) if scene['scene'] == scene_name]\n",
    "\n",
    "# Check if there are any available inputs for the selected scene\n",
    "if not scene_inputs:\n",
    "    print(f'No available inputs found for scene {scene_name}')\n",
    "else:\n",
    "    # Select a random input from that scene\n",
    "    random_input = random.choice(scene_inputs)\n",
    "\n",
    "    # Reshape the input to match the model's expected input shape\n",
    "    random_input = np.array(random_input).reshape(1, -1)\n",
    "\n",
    "    # Make a prediction\n",
    "    predicted_exposures = best_rf_bayes.predict(random_input)\n",
    "\n",
    "    # Get the ground-truth exposure times for the scene\n",
    "    ground_truth_exposures = next(scene['best_exposure_times'] for scene in scenes if scene['scene'] == scene_name)\n",
    "\n",
    "    # Calculate the percentage error for each exposure time\n",
    "    percentage_errors = [\n",
    "        abs((pred - gt) / gt) * 100 if gt != 0 else 0\n",
    "        for pred, gt in zip(predicted_exposures[0], ground_truth_exposures)\n",
    "    ]\n",
    "\n",
    "    # Print the predicted and ground-truth exposure times along with percentage errors\n",
    "    print(f'Predicted exposure times for scene {scene_name}: {predicted_exposures}')\n",
    "    print(f'Ground-truth exposure times for scene {scene_name}: {ground_truth_exposures}')\n",
    "    print(f'Percentage errors for each exposure time: {percentage_errors}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
